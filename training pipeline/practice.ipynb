{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41122,"status":"ok","timestamp":1690904423468,"user":{"displayName":"samir suraj","userId":"12538075674619197297"},"user_tz":-180},"id":"lx93xVNB9aqJ","outputId":"514539ad-2c27-423f-9de7-68c6a1d237af","scrolled":true},"outputs":[],"source":["!gdown 1seeIzLeGu8MZyCTnJx2nQpilKBHIcHvK #crs.txt\n","!gdown 1iHPYor7CKiJY95Attt2nEFtY8rrQD0oU #data.zip\n","!gdown 1gbU9CaSHfsDhALcGZd3hcbSpi9icPiLw #gaziera.zip\n","!gdown 1kPgjP5qSqseDZtfGseWuhiAc21BWm7pK #qdarif under sampled\n","\n","!gdown 1zRKg1MWjhGkeSzJNGFhhaUIIO-FljulR\n","!gdown 15Bpveev30iV46Ehlk5pQN5hHqqmW_Hdu\n","!gdown 1ZNYPklSzjYnD1fYtd9eYu0OIcWJmK6cK\n","!unzip data.zip\n","!pip install -q geopandas\n","!pip install -q contextily\n","!pip install -q mapclassify\n","!pip install -q streamlit-folium\n","!pip install -q folium\n","!pip install -q rioxarray\n","!mkdir curated_data\n","!mv gaziera_83_columns.csv curated_data/\n","!mv managil_83_columns.csv curated_data/\n","!mv qadarif_83_columns.csv curated_data/\n","!mv qadarif_83_columns_under_sampled.csv curated_data/"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1690903663070,"user":{"displayName":"samir suraj","userId":"12538075674619197297"},"user_tz":-180},"id":"-NMXjHNAAS9I"},"outputs":[],"source":["#some useful libraries and functions\n","\n","import time\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import train_test_split, cross_validate\n","from sklearn import metrics\n","from imblearn.under_sampling import RandomUnderSampler\n","import geopandas as gpd\n","import json\n","import matplotlib.pyplot as plt\n","import contextily as cx\n","from shapely.geometry import Polygon, Point\n","from pyproj import CRS\n","from shapely import wkt\n","from tqdm import tqdm\n","from matplotlib.colors import ListedColormap\n","from sklearn import tree\n","import  sklearn.feature_selection\n","from sklearn import feature_selection as fs\n","from sklearn import preprocessing as ps\n","from sklearn import decomposition as dc\n","import joblib\n","import numpy as np\n","from sklearn import ensemble as ens\n","from sklearn.pipeline import Pipeline\n","\n","\n","def convert_dataframe_to_geodataframe(df, crs):\n","    df['geometry'] = df['geometry'].apply(wkt.loads)\n","    gdf = gpd.GeoDataFrame(df, geometry=df.geometry, crs=crs)\n","    return gdf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1673,"status":"ok","timestamp":1690903669047,"user":{"displayName":"samir suraj","userId":"12538075674619197297"},"user_tz":-180},"id":"vNu3e8vbpRCp"},"outputs":[],"source":["import rioxarray as rx\n","import xarray as xr\n","import math\n","from sklearn.model_selection import GridSearchCV\n","\n","# The pipeline class which is the main output from the training pipline,\n","# The class has three main functions and an initiation function\n","\n","class PipelineW():\n","    # The init function takes a scikit learn pipeline layers array and a list of crops (anythign else is converted to other)\n","    def __init__(self, pipeline, cropnames=['Sorghum'], train=False):\n","        self.train=train\n","        self.pipeline=Pipeline(pipeline)\n","        self.cropnames=cropnames\n","        self.temp=None\n","        self.crs=crs\n","\n","    #The gen function takes the labels gdf from the downlowder -> gets the images downloaded -> and does some feature engineering\n","    # it then outputs the gdf in the format ready to be processes by the fit and predict functions.\n","\n","    def gen(self, labelsgdf=None, train=False):\n","        self.train=train\n","        gdf=self.readgdf(labelsgdf)\n","        return gdf\n","\n","    #Takes in the output form the gen function (train=true) and trains the pieplien model on it\n","    #basically the same as the scikit learn fit functin of the pipeline class\n","    def fit(self, gdf):\n","        Y=gdf.Crop_Type.values\n","        X=gdf.drop(['State', 'fieldID', 'Rainfed', 'Crop_Type', 'Year', 'y_bins', 'x_bins'], axis=1, errors='ignore')\n","        self.pipeline.fit(X, y=Y)\n","        return\n","\n","    #Takes in the output form the gen function (train=false) and predicts the class\n","    #basically the same as the scikit learn predict functin of the pipeline class\n","    def predict(self, X):\n","        predy=self.pipeline.predict(X)\n","        return predy\n","\n","    #Takes in the output form the gen function (train=false) and outputs the best model\n","    #basically the same as the scikit learn predict functin of the pipeline class\n","\n","    def selectbest(self,gdf, param_grid, pipe=False):\n","        Y=gdf.Crop_Type.values\n","        X=gdf.drop(['State', 'fieldID', 'Rainfed', 'Crop_Type', 'Year', 'y_bins', 'x_bins'], axis=1, errors='ignore')\n","        if not pipe:\n","            pipe=self.pipeline\n","        search = GridSearchCV(pipe, param_grid, n_jobs=4, verbose=4, scoring=['precision', 'recall','f1_micro'], refit='f1_micro')\n","        search.fit(X, Y)\n","        print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n","        print(search.best_params_)\n","        self.pipeline=search.best_estimator_\n","        return search\n","\n","    #Function that reads the images tiff files and formats them in required geogson format\n","        #[geometry, Crop_Type, Band_Date_pairs ...etc]\n","    def readgdf(self, labelsgdf):\n","        print('Reading tiff files')\n","        gdf=pd.DataFrame()\n","        labelsgdf=labelsgdf.sample(frac=1)\n","        gdfL=[]\n","        gdfl=[]\n","        for i, field in tqdm(labelsgdf.iterrows()):\n","            L=[]\n","\n","            for imgPath in field['imgPaths']:\n","\n","                nxr= rx.open_rasterio(imgPath)#, parse_coordinates=crs)\n","                L.append(nxr[:14,:,:])\n","            nxr=xr.concat(L, \"T\")\n","            ###group data\n","            nxr=self.group_xarr_max(nxr)\n","            ##################\n","            nxr.name='data'\n","            nxr=nxr.to_dataframe()\n","            nxr.drop('spatial_ref', axis=1,  inplace=True)\n","            nxr=nxr.unstack(level=['band', 'T'])\n","            nxr.reset_index(inplace=True)\n","            rows=nxr.shape[0]\n","            if self.train:\n","                nxr['Crop_Type']=[self.replace_label(field['Crop_Type'])]*rows\n","            nxr['State']=[field['State']]*rows\n","            nxr['Year']=[field['Year']]*rows\n","            nxr['Rainfed']=[field['Rainfed']]*rows\n","            nxr['fieldID']=[field['fieldID']]*rows\n","            if i==0:\n","                gdf=nxr\n","                continue\n","            gdfl.append(nxr)\n","            if i%200==0 or i == (len(labelsgdf)-1):\n","                gdfL.append(pd.concat(gdfl, axis=0))\n","                gdfl=[]\n","        gdf=pd.concat(gdfL, axis=0)\n","\n","#         geo=gpd.points_from_xy(gdf.x, gdf.y, crs=crs)\n","#         gdf=gpd.GeoDataFrame(gdf, geometry=geo)\n","#         self.temp=gdf\n","        print(gdf.shape)\n","        return gdf\n","\n","    def replace_label(self,crop_name):\n","        if crop_name is None:\n","            crop_name=0\n","        elif crop_name in self.cropnames:\n","            crop_name=self.cropnames.index(crop_name)+1\n","        else:\n","            crop_name=0\n","        return crop_name\n","\n","    def group_xarr(self, nxr):\n","        xlen=nxr.shape[3]\n","        binsx=math.ceil(xlen/3)\n","        ylen=nxr.shape[2]\n","        binsy=math.ceil(ylen/3)\n","\n","        nxrg=nxr.groupby_bins('x',binsx).median()\n","        nxrg=nxrg.groupby_bins('y',binsy).median()\n","\n","        return nxrg\n","\n","    def group_xarr_max(self, nxr):\n","        xlen=nxr.shape[3]\n","        binsx=math.ceil(xlen/5)\n","        ylen=nxr.shape[2]\n","        binsy=math.ceil(ylen/5)\n","\n","        nxrg=nxr.groupby_bins('x',binsx).max()\n","        nxrg=nxrg.groupby_bins('y',binsy).max()\n","\n","        return nxrg\n","\n","\n","# class group_transformer(BaseEstimator, TransformerMixin):\n","#     def __init__(self, fac=(3, 3)):\n","#         self.gdf=None\n","#         self.fac=fac\n","#         self.gdf=gpd.GeoDataFrame()\n","\n","#     def fit (self, X, y=None):\n","#         return\n","\n","#     def transform(self, X, y=None):\n","#         self.gdf['cluster']=list(zip(round(all_gdf.geometry.x*fac[0], fac[1])/fac[0],round(all_gdf.geometry.y*fac[0],fac[1])/fac[0]))\n","#         self.gdf=gpd.GeoDataFrame(self.gdf.groupby(by=['cluster','state', 'Crop_Type'], as_index=False).median(), geometry=self.gdf['geometry'])\n"]},{"cell_type":"markdown","metadata":{"id":"vR3NC7chpc3l"},"source":["<h1>Binary Sorghum DT NDVI PCA**<h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1690903671568,"user":{"displayName":"samir suraj","userId":"12538075674619197297"},"user_tz":-180},"id":"J8dEjqMXUlCz","outputId":"e22c9c3a-6341-42c5-fe61-33eaef33ac4a"},"outputs":[],"source":["%%time\n","with open('crs.txt', 'r') as file:\n","    crs = CRS.from_wkt(file.read())\n","# all_gdf=convert_dataframe_to_geodataframe(pd.read_csv(\"./curated_data/new_data.csv\"), crs).drop('Unnamed: 0', axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"elapsed":570,"status":"error","timestamp":1690903490107,"user":{"displayName":"samir suraj","userId":"12538075674619197297"},"user_tz":-180},"id":"QpKQAMDnpRCv","outputId":"40bf87f5-ca9a-43a2-ecf4-64f55c584cad"},"outputs":[],"source":["#load labels gdf passed from downloader\n","labelsgdf=joblib.load('labels.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIkdmxnzpRCw","outputId":"eb30dc6d-d7b3-4bab-9e73-075a05bb5811"},"outputs":[],"source":["#Setting pipeline layers\n","norm1=ps.MinMaxScaler()\n","kb1=fs.SelectKBest(score_func=fs.chi2, k=30)\n","cl1=ens.RandomForestClassifier(n_estimators=10, min_samples_leaf=2000) #max_depth = 3\n","\n","#initiating PieplineW class, by passing it pipeline layers array and cropnames\n","model1 = PipelineW([('minmax', norm1),('f_class_best', kb1), ('RF', cl1)], cropnames=['Sorghum'])\n","\n","#generates training geogson\n","gdf=model1.gen(labelsgdf, train=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9V9OmeDpRCy","outputId":"cfa60982-e1d9-4f9d-c72e-d29fab5487ae"},"outputs":[],"source":["#stores or loads featured gegoson files\n","\n","joblib.dump(gdf, 'gdfGrouped5Sorghum.joblib')\n","#gdf=joblib.load('gdfGroupedCotton.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wampfZRIpRC1"},"outputs":[],"source":["#fitting pipeline\n","\n","model1.fit(gdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zo6Hm03vpRC2","outputId":"14997b46-474b-49b3-c745-80db78fd5e32"},"outputs":[],"source":["#Features selection set up\n","\n","#Setting pipeline layers\n","norm1=ps.MinMaxScaler()\n","kb1=fs.SelectKBest(score_func=fs.chi2) #score_func=fs.f_classif, k=30\n","cl1=ens.RandomForestClassifier(n_estimators=10) #max_depth = 3 #n_estimators=10, min_samples_leaf=2000\n","model1 = PipelineW([('minmax', norm1),('f_class_best', kb1), ('RF', cl1)], cropnames=['Cotton'])\n","\n","#settign parametere to search\n","param_grid = {\n","    #\"f_class_best__score_func\": [fs.f_classif, fs.chi2],\n","    \"f_class_best__k\":[20,30,40],\n","    #\"RF__max_depth\": [3,5,7,9,11],\n","    #\"RF__n_estimators\":[10,15,20],\n","    \"RF__min_samples_leaf\":[1000,2000,3000],\n","    \"RF__criterion\":[\"gini\", \"entropy\", \"log_loss\"],\n","}\n","\n","\n","#finding the best fitting model\n","model1.selectbest(gdf,param_grid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqpI6S2BpRC3"},"outputs":[],"source":["#predicting classes\n","preds=model1.predict(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffmmO61kpRC4"},"outputs":[],"source":["#testing model\n","print(metrics.classification_report(Y, preds, zero_division=0))\n","cm=metrics.confusion_matrix(Y, preds, labels=(1, 0))\n","print(cm)\n","metrics.ConfusionMatrixDisplay(cm).plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPUul89lpRC5"},"outputs":[],"source":["#storing or loading model\n","\n","joblib.dump(model1, './groupRFoptCotton.joblib')\n","#model1=joblib.load('./groupRFoptCotton.joblib')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}
